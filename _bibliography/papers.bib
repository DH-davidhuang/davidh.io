---
---
@inproceedings{verma2025measuringgeneralintelligencegenerated,
      title={Measuring General Intelligence with Generated Games}, 
      author={Verma, Vivek and Huang, David and Chen, William and Klein, Dan and Tomlin, Nicholas},
      year={2025},
      eprint={2505.07215},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.07215}, 
      pdf={https://arxiv.org/abs/2505.07215},
      code={https://github.com/vivek3141/gg-bench},
      booktitle={Under submission at ICLR 2026},
      preview = {better_logo_measuring.png},
      selected = {true},
      bibtex_show = {true},
      abstract      = {We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.}
}

@inproceedings{huang_stronger_2024,
  abbr        = {NAACL},
  title       = {Stronger Universal and Transfer Attacks by Suppressing Refusals},
  booktitle   = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  author      = {Huang, David and Shah, Avidan and Araujo, Alexandre and Wagner, David and Sitawarin, Chawin},
  abstract    = {Making large language models (LLMs) safe for mass deployment is a complex and ongoing challenge. Efforts have focused on aligning models to human prefer- ences (RLHF) in order to prevent malicious uses, essentially embedding a “safety feature” into the model’s parameters. The Greedy Coordinate Gradient (GCG) algorithm (Zou et al., 2023b) emerges as one of the most popular automated jail- breaks, an attack that circumvents this safety training. So far, it is believed that these optimization-based attacks (unlike hand-crafted ones) are sample-specific. To make the automated jailbreak universal and transferable, they require incorporating multiple samples and models into the objective function. Contrary to this belief, we find that the adversarial prompts discovered by such optimizers are inherently prompt-universal and transferable, even when optimized on a single model and a single harmful request. To further amplify this phenomenon, we introduce IRIS, a new objective to these optimizers to explicitly deactivate the safety feature to create an even stronger universal and transferable attack. Without requiring a large number of queries or accessing output token probabilities, our transfer attack, optimized on Llama-3, achieves a 25% success rate against the state-of-the-art Circuit Breaker defense (Zou et al., 2024), compared to 2.5% by white-box GCG. Crucially, our universal attack method also attains state-of-the-art test-set transfer rates on frontier models: GPT-3.5-Turbo (90%), GPT-4o-mini (86%), GPT-4o (76%), o1-mini (54%), and o1-preview (48%).},
  year        = {2025},
  address     = {Mexico City, Mexico},
  note        = {Also appeared in Neurips Safe Generative {{AI}} Workshop 2024.},
  month       = apr,
  url         = {https://aclanthology.org/2025.naacl-long.302/},
  pdf         = {https://aclanthology.org/2025.naacl-long.302.pdf},
  bibtex_show = {true},
  preview     = {iris_thumbnail.png},
  selected    = {true},
}

@inproceedings{sitawarin_defending_2024,
  title         = {PubDef: Defending against Transfer Attacks from Public Models},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  author        = {Sitawarin, Chawin and Chang{${*}$}, Jaewon and Huang{${*}$}, David and Altoyan, Wesson and Wagner, David},
  year          = {2024},
  month         = jan,
  copyright     = {CC0 1.0 Universal Public Domain Dedication},
  url           = {https://openreview.net/forum?id=Tvwf4Vsi5F},
  abstract      = {Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other words, the white-box threat model, religiously assumed by a large majority of the past literature, is unrealistic. In this paper, we propose a new practical threat model where the adversary relies on transfer attacks through publicly available surrogate models. We argue that this setting will become the most prevalent for security-sensitive applications in the future. We evaluate the transfer attacks in this setting and propose a specialized defense method based on a game-theoretic perspective. The defenses are evaluated under 24 public models and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and ImageNet). Under this threat model, our defense, PubDef, outperforms the state-of-the-art white-box adversarial training by a large margin with almost no loss in the normal accuracy. For instance, on ImageNet, our defense achieves 62% accuracy under the strongest transfer attack vs only 36% of the best adversarially trained model. Its accuracy when not under attack is only 2% lower than that of an undefended model (78% vs 80%).},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,notion},
  code          = {https://github.com/wagner-group/pubdef},
  bibtex_show   = {true},
  abstract      = {Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear way, under many distribution shifts. The latter enables the prediction of OOD robustness from ID robustness. Based on this, we are able to predict the upper limit of OOD robustness for existing robust training schemes. The results suggest that achieving OOD robustness requires designing novel methods beyond the conventional ones. Last, we discover that extra data, data augmentation, advanced model architectures and particular regularization approaches can improve OOD robustness. Noticeably, the discovered training schemes, compared to the baseline, exhibit dramatically higher robustness under threat shift while keeping high ID robustness, demonstrating new promising solutions for robustness against both multi-attack and unforeseen attacks.},
  abbr          = {ICLR},
  arxiv         = {2310.17645},
  preview       = {pubdef_thumbnail.png},
  pdf           = {https://openreview.net/forum?id=Tvwf4Vsi5F},
  website       = {https://wagner-group.github.io/projects/pubdef/index.html},
  talk          = {https://recorder-v3.slideslive.com/?share=92103&s=183329c8-37ce-42c4-a41a-fff6998c8055},
  slides        = {pubdef_iclr2024_slides.pdf},
  selected      = {true},
}

@inproceedings{chen_robo_dm_2025,
  title       = {Robo-DM: Efficient Robot Big Data Management},
  author      = {Kaiyuan Chen and Letian Fu and Huang{${*}$}, David and Zhang{${*}$}, Y. and Chen, L. and Huang, H. and Hari, K. and Balakrishna, A. and Sanketi, P. and Kubiatowicz, J. and Goldberg, K.},
  abstract    = {Recent work suggests that very large datasets of
teleoperated robot demonstrations can train transformer-based
models that have the potential to generalize to new scenes,
robots, and tasks. However, curating, distributing, and loading
large datasets of robot trajectories, which typically consist of
video, textual, and numerical modalities - including streams from
multiple cameras - remains challenging. We propose Robo-DM,
an efficient cloud-based data management toolkit for collecting,
sharing, and learning with robot data. With Robo-DM, robot
datasets are stored in a self-contained format with Extensible
Binary Meta Language (EBML). Robo-DM reduces the size
of robot trajectory data, transfer costs, and data load time
during training. In particular, compared to the RLDS format
used in OXE datasets, Robo-DM’s compression saves space by
up to 70x (lossy) and 3.5x (lossless). Robo-DM also accelerates
data retrieval by load-balancing video decoding with memory-
mapped decoding caches. Compared to LeRobot, a framework that also uses lossy video compression, Robo-DM is up to 50x
faster. In fine-tuning Octo, a transformer-based robot policy
with 73k episodes with RT-1 data, Robo-DM does not incur any
loss at training performance. We physically evaluate a model
trained by Robo-DM with lossy compression, a pick-and-place
task, and In-Context Robot Transformer. Robo-DM uses 75x
compression of the original dataset and does not suffer any
reduction in downstream task accuracy. Code and evaluation
scripts can be found on website
.},
  booktitle   = {ICRA 2025: IEEE International Conference on Robotics and Automation},
  year        = {2025},
  award = {Best Paper Award on Robot Learning (4153 Papers Submitted)                    }, 
  award_name = {IEEE ICRA Best Paper Award on Robot Learning},
  url         = {https://arxiv.org/pdf/2505.15558},
  pdf         = {https://arxiv.org/pdf/2505.15558},
  code        = {https://github.com/BerkeleyAutomation/fog_x},
  video       = {https://drive.google.com/file/d/1onSS_wmcZ6-JiR3ebR5BMDzEosZaL6X6/view},
  bibtex_show = {true},
  abbr = {ICRA},
  preview     = {robo_thumbnail.png},
  selected    = {true}
}

@inproceedings{alignment_2025,
  abbr        = {ICML},
  author      = {Zhao{${*}$}, Xuandong and Cai{${*}$}, Will and Shi, Tianneng and Huang, David and Lin, Licong and Mei, Song and Song, Dawn},
  title       = {Improving LLM Safety Alignment with Dual-Objective Optimization},
  booktitle   = {Forty-second International Conference on Machine Learning},
  abstract    = {Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct Preference Optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research further suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment.},
  year        = {2025},
  month       = May,
  preview     = {DOOR_diagram.png},
  bibtex_show = {true},
  url         = {https://arxiv.org/pdf/2503.03710},
  pdf         = {https://arxiv.org/pdf/2503.03710},
  code        = {https://github.com/wicai24/DOOR-Alignment},
}