---
---

@inproceedings{huang_stronger_2024,
  abbr        = {Preprint},
  title       = {Stronger Universal and Transfer Attacks by Suppressing Refusals},
  booktitle   = {Neurips Safe Generative {{AI}} Workshop 2024},
  author      = {Huang, David and Shah, Avidan and Araujo, Alexandre and Wagner, David and Sitawarin, Chawin},
  abstract    = {Making large language models (LLMs) safe for mass deployment is a complex and ongoing challenge. Efforts have focused on aligning models to human preferences (RLHF) in order to prevent malicious uses, essentially embedding a ``safety feature'' into the model's parameters. The Greedy Coordinate Gradient (GCG) algorithm (Zou et al., 2023b) emerges as one of the most popular automated jailbreaks, an attack that circumvents this safety training. So far, it is believed that these optimization-based attacks are sample-specific as opposed to hand-crafted ones. To make the automated jailbreak universal and transferable, they require incorporating multiple samples and models into the objective function. Contrary to this belief, we find that the adversarial prompts discovered by such optimizers are inherently prompt-universal and transferable, even when optimized on a single model and a single harmful request. To further amplify this phenomenon, we introduce a new objective to these optimizers to explicitly deactivate the safety feature to create an even stronger universal and transferable attack. Without requiring a large number of queries or accessing output token probabilities, our transfer attack, optimized on Llama-3, achieves a 92% success rate against the state-of-the-art Circuit Breaker defense, compared to 2.5% by white-box GCG. Crucially, our method also attains state-of-the-art transfer rates on frontier models: GPT-3.5-Turbo (96%), GPT-4o (82%), and GPT-4o-mini (88%).},
  year        = {2024},
  month       = oct,
  note        = {Full version is under submission and review at 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics.},
  url         = {https://openreview.net/forum?id=eIBWRAbhND},
  pdf         = {https://openreview.net/forum?id=eIBWRAbhND},
  bibtex_show = {true},
  preview     = {iris_thumbnail.png},
  selected    = {true},
}

@inproceedings{sitawarin_defending_2024,
  title         = {PubDef: Defending against Transfer Attacks from Public Models},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  author        = {Sitawarin, Chawin and Chang{${*}$}, Jaewon and Huang{${*}$}, David and Altoyan, Wesson and Wagner, David},
  year          = {2024},
  month         = jan,
  copyright     = {CC0 1.0 Universal Public Domain Dedication},
  url           = {https://openreview.net/forum?id=Tvwf4Vsi5F},
  abstract      = {Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other words, the white-box threat model, religiously assumed by a large majority of the past literature, is unrealistic. In this paper, we propose a new practical threat model where the adversary relies on transfer attacks through publicly available surrogate models. We argue that this setting will become the most prevalent for security-sensitive applications in the future. We evaluate the transfer attacks in this setting and propose a specialized defense method based on a game-theoretic perspective. The defenses are evaluated under 24 public models and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and ImageNet). Under this threat model, our defense, PubDef, outperforms the state-of-the-art white-box adversarial training by a large margin with almost no loss in the normal accuracy. For instance, on ImageNet, our defense achieves 62% accuracy under the strongest transfer attack vs only 36% of the best adversarially trained model. Its accuracy when not under attack is only 2% lower than that of an undefended model (78% vs 80%).},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,notion},
  code          = {https://github.com/wagner-group/pubdef},
  bibtex_show   = {true},
  abstract      = {Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear way, under many distribution shifts. The latter enables the prediction of OOD robustness from ID robustness. Based on this, we are able to predict the upper limit of OOD robustness for existing robust training schemes. The results suggest that achieving OOD robustness requires designing novel methods beyond the conventional ones. Last, we discover that extra data, data augmentation, advanced model architectures and particular regularization approaches can improve OOD robustness. Noticeably, the discovered training schemes, compared to the baseline, exhibit dramatically higher robustness under threat shift while keeping high ID robustness, demonstrating new promising solutions for robustness against both multi-attack and unforeseen attacks.},
  abbr          = {ICLR},
  arxiv         = {2310.17645},
  preview       = {pubdef_thumbnail.png},
  pdf           = {https://openreview.net/forum?id=Tvwf4Vsi5F},
  website       = {https://wagner-group.github.io/projects/pubdef/index.html},
  talk          = {https://recorder-v3.slideslive.com/?share=92103&s=183329c8-37ce-42c4-a41a-fff6998c8055},
  slides        = {pubdef_iclr2024_slides.pdf},
  selected      = {true},
}

@inproceedings{chen_robo_dm_2025,
  abbr        = {Under Review},
  title       = {Robo-DM: Efficient Robot Big Data Management},
  author      = {Kaiyuan Chen and Letian Fu and Huang, David and Zhang, Y. and Chen, L. and Huang, H. and Hari, K. and Balakrishna, A. and Sanketi, P. and Kubiatowicz, J. and Goldberg, K.},
  abstract    = {Recent work suggests that very large datasets of
teleoperated robot demonstrations can train transformer-based
models that have the potential to generalize to new scenes,
robots, and tasks. However, curating, distributing, and loading
large datasets of robot trajectories, which typically consist of
video, textual, and numerical modalities - including streams from
multiple cameras - remains challenging. We propose Robo-DM,
an efficient cloud-based data management toolkit for collecting,
sharing, and learning with robot data. With Robo-DM, robot
datasets are stored in a self-contained format with Extensible
Binary Meta Language (EBML). Robo-DM reduces the size
of robot trajectory data, transfer costs, and data load time
during training. In particular, compared to the RLDS format
used in OXE datasets, Robo-DMâ€™s compression saves space by
up to 70x (lossy) and 3.5x (lossless). Robo-DM also accelerates
data retrieval by load-balancing video decoding with memory-
mapped decoding caches. Compared to LeRobot, a framework that also uses lossy video compression, Robo-DM is up to 50x
faster. In fine-tuning Octo, a transformer-based robot policy
with 73k episodes with RT-1 data, Robo-DM does not incur any
loss at training performance. We physically evaluate a model
trained by Robo-DM with lossy compression, a pick-and-place
task, and In-Context Robot Transformer. Robo-DM uses 75x
compression of the original dataset and does not suffer any
reduction in downstream task accuracy. Code and evaluation
scripts can be found on website
.},
  booktitle   = {Full version is under submission and review at ICRA 2025: IEEE International Conference on Robotics and Automation},
  year        = {2025},
  url         = {https://drive.google.com/file/d/1CvZ80ShyYs3tCGPdjLfNHbsy-engqImw/view?usp=sharing},
  pdf         = {https://drive.google.com/file/d/1CvZ80ShyYs3tCGPdjLfNHbsy-engqImw/view?usp=sharing},
  code        = {https://github.com/BerkeleyAutomation/fog_x},
  video       = {https://drive.google.com/file/d/1onSS_wmcZ6-JiR3ebR5BMDzEosZaL6X6/view?usp=sharing},
  bibtex_show = {true},
  selected    = {true}
}
